{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee3d7fd-c0c3-47bf-820b-cb2b94e70908",
   "metadata": {},
   "source": [
    "# 0. 設定 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458744cd-e2db-463e-a419-0ad9e6145e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bdse111.example.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>maggie</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f965022f790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3706663a-2717-490c-8787-adaf1af62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import gc\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import array_max\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.pandas.config import set_option, reset_option\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e94b47e-b307-4b85-9c1e-85056953d516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f5416-468d-48ff-9a59-c12a5ca92041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'512m'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.kryoserializer.buffer.max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211e0ab-6b6c-4e2f-a7f0-48dde3c45262",
   "metadata": {},
   "source": [
    "# 1. 讀取檔案 => tran_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ed44a-369c-4a67-a68a-aee7f5bf83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案\n",
    "# customers = ps.read_parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = ps.read_parquet('/user/HM_parquet/articles.parquet')\n",
    "tran_ps = ps.read_parquet('/user/HM_parquet/transactions_train.parquet').drop(['price', 'sales_channel_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881aeab-fa63-4d2f-8f4b-fc371e3ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_ps.set_index('t_dat',inplace=True)\n",
    "tran_ps['start_test'] = ''\n",
    "tran_ps['split_id'] = ''\n",
    "tran_ps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66614e-1b55-43e3-a632-42d05d0be6ef",
   "metadata": {},
   "source": [
    "# 2. 作時間分割 tran_ps => split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079e109-bcd4-4284-a78c-e8f4b6fbcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data,train_period=30, test_period=7, stride=30,show_progress=False):\n",
    "    \n",
    "    split_data = ps.DataFrame(columns = ['t_dat','customer_id', 'article_id', 'split_id', 'start_test']).set_index('t_dat',inplace=True)\n",
    "\n",
    "    end_test = data.index.max()\n",
    "    start_test = end_test - relativedelta(days=test_period)\n",
    "    start_train = start_test - relativedelta(days=train_period)\n",
    "    split_id=0\n",
    "\n",
    "    while start_train >= data.index.min():\n",
    "\n",
    "        df = data.loc[start_train:end_test]\n",
    "        df['start_test']=start_test\n",
    "        df['split_id'] = split_id\n",
    "        split_data = ps.concat([split_data,df])\n",
    "\n",
    "        if(show_progress):\n",
    "            print(\"Split_id:\",split_id,\", Train period:\",start_train,\"-\" , start_test, \", test period\", start_test, \"-\", end_test)\n",
    "\n",
    "        # update dates:\n",
    "        end_test = end_test - relativedelta(days=stride)\n",
    "        start_test = end_test - relativedelta(days=test_period)\n",
    "        start_train = start_test - relativedelta(days=train_period)\n",
    "        split_id += 1\n",
    "    \n",
    "    return split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e573f4-6c40-4cb0-a04c-2230ee9738ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data, split_id = split(tran_ps,30,7,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7446fe7-8a98-4d3b-95df-804cf4a96186",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ff57c-714a-4013-abba-83622e50ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87169e28-4c31-4acf-a0b0-d36649c0c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e081df-ce79-4d81-a63a-834864bdebc2",
   "metadata": {},
   "source": [
    "# 3. 製作參數表 split_id => para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af31c0-2956-45eb-a010-48ba1131c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作參數表 paras_grid\n",
    "from itertools import product\n",
    "\n",
    "paras = list(\n",
    "    product(\n",
    "        [25,50,100,150,200],\n",
    "        [20,30,40,50],\n",
    "        [0.01]\n",
    "    )\n",
    ")\n",
    "paras_grid = ps.DataFrame(paras,columns= ['n_factors','n_epochs','reg_all'])\n",
    "paras_grid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e5a00-8133-411d-ad2f-d8555769b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作 split_id 表\n",
    "split_id_ps = ps.DataFrame({'split_id': range(split_id)})\n",
    "split_id_ps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042a73-b886-4c89-bc11-fe0d58390b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 paras_grid 與 split_id 做 cross join\n",
    "paras_grid['key'] = 1\n",
    "split_id_ps['key'] = 1\n",
    "\n",
    "para_cross_split = ps.merge(paras_grid, split_id_ps, on ='key').drop('key')\n",
    "del split_id_ps\n",
    "len(para_cross_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b85df-084a-4fe9-92ab-593bd6a3735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 cross join 後的表新增遞增的 group_id 欄位，之後要用來做 pandas_udf 的 groupby\n",
    "para_cross_split['group_id'] = 0\n",
    "para_cross_split['group_id'] = np.arange(len(para_cross_split)).tolist()\n",
    "para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927c99b-a1fd-4bca-b668-bb5239402a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_cross_split.to_parquet('/user/HM_parquet/SVD_model/para_cross_split.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e237f52-9033-4e3d-bca3-01f15621a19a",
   "metadata": {},
   "source": [
    "# 4. join 參數表和資料表 split_data, para_cross_split => join_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cc8e2-ab37-4c93-9d2c-94ba36426f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data = split_data.join(para_cross_split.set_index('split_id'), on='split_id')\n",
    "join_data.set_index('t_dat',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0e02c-c4c8-4c35-b05d-2a3773404c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cace0df-e8bc-4310-ad49-4c8aefd26373",
   "metadata": {},
   "outputs": [],
   "source": [
    "39919883*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a348c3-df01-4ea8-8c4f-1831f2a73da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c57b7-32ce-472e-ba18-6a39d15d5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_data.to_parquet('/user/HM_parquet/SVD_model/join_data30.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5c883-13c7-4909-b4d5-2f6b6c9906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除用不到的資料表\n",
    "del split_data, para_cross_split\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347efaa-1f3b-4ca9-a904-6f13017347f1",
   "metadata": {},
   "source": [
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567ccf7-b7a6-49f0-b995-d48fe917c7bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 讀取資料表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab23a46-83fa-4cbd-9bd9-080f004a65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "798397660"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option('header','true').parquet('/user/HM_parquet/SVD_model/join_data30.parquet')\n",
    "# join_data.set_index('t_dat',inplace=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21d9cdb-574b-45a1-8386-9eb84b26a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(split_id=19, t_dat=datetime.date(2019, 1, 24), customer_id='000253f6914890557a88d0b91288ce85fae9332dac43ee5445c33e3891df6fd3', article_id=660599007, start_test=datetime.date(2019, 2, 23), n_factors=25, n_epochs=30, reg_all=0.01, group_id=43),\n",
       " Row(split_id=19, t_dat=datetime.date(2019, 1, 24), customer_id='000253f6914890557a88d0b91288ce85fae9332dac43ee5445c33e3891df6fd3', article_id=660599007, start_test=datetime.date(2019, 2, 23), n_factors=25, n_epochs=40, reg_all=0.01, group_id=67),\n",
       " Row(split_id=19, t_dat=datetime.date(2019, 1, 24), customer_id='000253f6914890557a88d0b91288ce85fae9332dac43ee5445c33e3891df6fd3', article_id=660599007, start_test=datetime.date(2019, 2, 23), n_factors=25, n_epochs=50, reg_all=0.01, group_id=91),\n",
       " Row(split_id=19, t_dat=datetime.date(2019, 1, 24), customer_id='000253f6914890557a88d0b91288ce85fae9332dac43ee5445c33e3891df6fd3', article_id=660599007, start_test=datetime.date(2019, 2, 23), n_factors=50, n_epochs=20, reg_all=0.01, group_id=115),\n",
       " Row(split_id=19, t_dat=datetime.date(2019, 1, 24), customer_id='000253f6914890557a88d0b91288ce85fae9332dac43ee5445c33e3891df6fd3', article_id=660599007, start_test=datetime.date(2019, 2, 23), n_factors=50, n_epochs=30, reg_all=0.01, group_id=139)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a46e2e-743b-4096-83d9-2ead5d1d7174",
   "metadata": {
    "tags": []
   },
   "source": [
    "# surpriseSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a81400-b881-4dff-9e0f-7b5f542b3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVDpp,SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import average_precision as metrics\n",
    "# import ml_metrics as metrics\n",
    "\n",
    "class surpriseSVD():\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def get_top_n(self, predictions, n=12):\n",
    "        \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "        Args:\n",
    "            predictions(list of Prediction objects): The list of predictions, as\n",
    "                returned by the test method of an algorithm.\n",
    "            n(int): The number of recommendation to output for each user. Default\n",
    "                is 10.\n",
    "        Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "            [(raw item id, rating estimation), ...] of size n.\n",
    "        \"\"\"\n",
    "\n",
    "        # First map the predictions to each user.\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est))\n",
    "\n",
    "        # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "\n",
    "        return top_n\n",
    "\n",
    "    def get_set(self,df):\n",
    "        reader = Reader(rating_scale=(1, 500))\n",
    "        data_set = Dataset.load_from_df(df[['customer_id','article_id','rating']], reader)\n",
    "        return data_set\n",
    "\n",
    "    def get_rating_set(self,df):\n",
    "        rating = df[['customer_id','article_id','price']].groupby(['customer_id','article_id']).count().reset_index()\n",
    "        rating.columns = ['customer_id','article_id','rating']\n",
    "        rating_set = self.get_set(rating)\n",
    "        return rating_set\n",
    "\n",
    "\n",
    "    def train_SVD(self, train_data, test_data, paras={}):\n",
    "\n",
    "        ## 讀取評分資料為surprise可以訓練的格式\n",
    "        trainset = self.get_rating_set(train_data)\n",
    "        testset = self.get_rating_set(test_data)\n",
    "\n",
    "        ## rmse 需要的資料\n",
    "        testset2 = [testset.df.loc[i].to_list() for i in range(len(testset.df))]\n",
    "\n",
    "        ## map@k testing 需要產的資料\n",
    "        test_data.loc[:,'rating']=0\n",
    "        test_processed = self.get_set(test_data)\n",
    "        NA, test2 = train_test_split(test_processed, test_size=1.0)\n",
    "\n",
    "        # ======= 消費者的實際購買清單 =======\n",
    "        test_data['article_id'] = test_data['article_id'].astype('str')\n",
    "        test_uni = test_data.drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "        buy_n = test_uni[['customer_id','article_id']].groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "        cust_actual_list = []\n",
    "        for uid, user_ratings in buy_n.items():\n",
    "            cust_pred_tuple = (uid, [iid for iid in user_ratings])\n",
    "            cust_actual_list.append(cust_pred_tuple)\n",
    "\n",
    "        # ======= 訓練 SVD 模型 =======\n",
    "        algo = SVD(random_state=42,**paras)\n",
    "\n",
    "        # 訓練模型\n",
    "        algo.fit(trainset.build_full_trainset())\n",
    "\n",
    "        ##### rmse #####\n",
    "        predictions = algo.test(testset2)\n",
    "        rmse = accuracy.rmse(predictions)\n",
    "\n",
    "        ##### map@k #####\n",
    "        predictions_map = algo.test(test2)\n",
    "        # est = [i.est for i in predictions_map] \n",
    "\n",
    "        ##  消費者的預測清單 \n",
    "        top_n = self.get_top_n(predictions=predictions_map, n=12)\n",
    "\n",
    "        cust_pred_list = []\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            cust_pred_tuple = (uid, [str(iid) for (iid, _) in user_ratings])\n",
    "            cust_pred_list.append(cust_pred_tuple)\n",
    "\n",
    "        final_list = list(zip(cust_actual_list, cust_pred_list))\n",
    "\n",
    "        # map@k計算 \n",
    "        mapk_list = []\n",
    "        for i in range(len(final_list)):\n",
    "            map_k = metrics.mapk([final_list[i][0][1]],[final_list[i][1][1]],12)\n",
    "            mapk_list.append(map_k)\n",
    "\n",
    "        map_k = sum(mapk_list)/len(mapk_list)\n",
    "\n",
    "        return rmse, map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6af06-e9bb-4765-98eb-eef519527caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f6e8a-fce2-4bf2-835e-e0e355aa914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = df.where(\"group_id == 43\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bb3b4b-df6b-41b0-8cf5-68904b3ca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf['t_dat'] = pd.to_datetime(pdf.t_dat)\n",
    "# pdf.set_index('t_dat',inplace=True, drop=False)\n",
    "# pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bace2342-f05b-443b-bd35-b37c2cc3e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split_hyperparameter_search(data):\n",
    "    paras = {\n",
    "        'n_factors':data.n_factors.values[0], \n",
    "        'n_epochs':data.n_epochs.values[0], \n",
    "        'reg_all':data.reg_all.values[0]\n",
    "    }\n",
    "    \n",
    "    data['t_dat'] = pd.to_datetime(data.t_dat)\n",
    "    data.set_index('t_dat',inplace=True, drop=False)\n",
    "    dataTrain = data.first('22D')\n",
    "    dataTest = data.last('6D')\n",
    "    \n",
    "    model = surpriseSVD()\n",
    "    rmse, map12 = model.train_SVD(dataTrain, dataTest, paras)\n",
    "    \n",
    "    paras.update({\n",
    "        'start_test' : data.start_test.values[0],\n",
    "        'rmse' : rmse,\n",
    "        'map12' : map12\n",
    "    })\n",
    "    \n",
    "    results = pd.DataFrame([paras])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc5960d-e3cf-4b6e-b757-75cd9f34fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DateType, DoubleType, FloatType, IntegerType, StringType, StructField, StructType\n",
    ")\n",
    "# pandas_udf\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('start_test', DateType(),True),\n",
    "        StructField(\"n_factors\", IntegerType(), True),\n",
    "        StructField(\"n_epochs\", IntegerType(), True),\n",
    "        StructField('reg_all', FloatType(),True),\n",
    "        StructField('rmse', FloatType(),True),\n",
    "        StructField('map12', FloatType(),True),\n",
    "     ]\n",
    ")\n",
    "\n",
    "results = df.groupby('group_id').applyInPandas(time_split_hyperparameter_search, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1971d1e4-8493-40c0-8382-80e13c4bc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:17:04,412 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.0 (TID 65) (bdse74.example.com executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:07,157 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.0 (TID 58) (bdse106.example.com executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000007/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:08,039 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.0 (TID 71) (bdse137.example.com executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000002/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:09,024 WARN scheduler.TaskSetManager: Lost task 19.0 in stage 7.0 (TID 77) (bdse42.example.com executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000009/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:11,601 WARN scheduler.TaskSetManager: Lost task 24.0 in stage 7.0 (TID 78) (bdse75.example.com executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000010/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:12,183 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 59) (bdse92.example.com executor 9): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000011/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:14,557 WARN scheduler.TaskSetManager: Lost task 3.1 in stage 7.0 (TID 85) (bdse74.example.com executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n",
      "    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'surprise'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-30 19:17:16,181 ERROR scheduler.TaskSetManager: Task 9 in stage 7.0 failed 4 times; aborting job\n",
      "2022-03-30 19:17:16,211 WARN scheduler.TaskSetManager: Lost task 3.2 in stage 7.0 (TID 108) (bdse137.example.com executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,214 WARN scheduler.TaskSetManager: Lost task 22.2 in stage 7.0 (TID 106) (bdse137.example.com executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,215 WARN scheduler.TaskSetManager: Lost task 21.2 in stage 7.0 (TID 103) (bdse106.example.com executor 5): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,224 WARN scheduler.TaskSetManager: Lost task 7.3 in stage 7.0 (TID 105) (bdse42.example.com executor 7): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,225 WARN scheduler.TaskSetManager: Lost task 34.2 in stage 7.0 (TID 110) (bdse74.example.com executor 3): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,226 WARN scheduler.TaskSetManager: Lost task 18.2 in stage 7.0 (TID 112) (bdse74.example.com executor 3): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,251 WARN scheduler.TaskSetManager: Lost task 10.3 in stage 7.0 (TID 111) (bdse92.example.com executor 9): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,428 WARN scheduler.TaskSetManager: Lost task 31.0 in stage 7.0 (TID 74) (bdse90.example.com executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,430 WARN scheduler.TaskSetManager: Lost task 17.0 in stage 7.0 (TID 63) (bdse90.example.com executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,560 WARN scheduler.TaskSetManager: Lost task 19.2 in stage 7.0 (TID 107) (bdse75.example.com executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:16,697 WARN scheduler.TaskSetManager: Lost task 35.0 in stage 7.0 (TID 102) (bdse106.example.com executor 5): TaskKilled (Stage cancelled)\n",
      "[Stage 7:>                                                       (0 + 11) / 181]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'surprise'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:680\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 423, in read_udfs\n    arg_offsets, f = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/hdpfs/nm-local-dir/usercache/hadoop/appcache/application_1648620987516_0008/container_e26_1648620987516_0008_01_000005/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'surprise'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 19:17:27,308 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.0 (TID 61) (bdse89.example.com executor 6): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:27,393 WARN scheduler.TaskSetManager: Lost task 14.0 in stage 7.0 (TID 72) (bdse89.example.com executor 6): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:27,962 WARN scheduler.TaskSetManager: Lost task 39.0 in stage 7.0 (TID 93) (bdse75.example.com executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:28,467 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 7.0 (TID 92) (bdse42.example.com executor 7): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:28,515 WARN scheduler.TaskSetManager: Lost task 2.1 in stage 7.0 (TID 109) (bdse92.example.com executor 9): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:32,140 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.0 (TID 64) (bdse108.example.com executor 10): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:32,146 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 7.0 (TID 75) (bdse108.example.com executor 10): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:32,923 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.0 (TID 68) (bdse109.example.com executor 11): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:32,931 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 7.0 (TID 57) (bdse109.example.com executor 11): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:33,322 WARN scheduler.TaskSetManager: Lost task 20.0 in stage 7.0 (TID 73) (bdse91.example.com executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-30 19:17:33,327 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 7.0 (TID 62) (bdse91.example.com executor 2): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac57e2-5202-4de7-b9af-2aa7e70d1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.parquet('/user/HM_parquet/SVD_model/para_30.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5348c4-3990-44eb-84b9-2eed404cc61b",
   "metadata": {},
   "source": [
    "# (DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b996c3-1b5e-4f11-8776-c5a589f457b2",
   "metadata": {},
   "source": [
    "## 1. 讀取檔案 /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431f87d-52d0-4c07-9609-ac6e977ea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀取檔案\n",
    "# customers = spark.read.option('header','true').parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = spark.read.option('header','true').parquet('/user/HM_parquet/articles.parquet')\n",
    "# transactions = spark.read.option('header','true').parquet('/user/HM_parquet/transactions_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a841413-2d9a-4f01-ba53-5d301b39082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012b563-2eb7-40c8-8d2a-39a119f9e288",
   "metadata": {},
   "source": [
    "## 2. 將customer_id(字串)轉為customer_index(整數) /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634903ba-7100-4a88-9f24-7becc2e1c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將customers的customer_id轉為數字(buffer要增加到512m)\n",
    "# toIndex = StringIndexer(inputCol=\"customer_id\", outputCol=\"customer_index\").fit(customers)\n",
    "# customers = toIndex.transform(customers)\n",
    "# # customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875dbbe-f263-4550-bca3-79c887a4901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將transactions的customer_id轉為數字\n",
    "# transactions = toIndex.transform(transactions)\n",
    "# # transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64ded3-e050-46e5-a4c2-cebf60cb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
